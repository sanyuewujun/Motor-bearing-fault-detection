{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c2cedd",
   "metadata": {
    "papermill": {
     "duration": 0.004501,
     "end_time": "2025-10-16T01:58:10.447861",
     "exception": false,
     "start_time": "2025-10-16T01:58:10.443360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1.导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6428aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T01:58:10.456574Z",
     "iopub.status.busy": "2025-10-16T01:58:10.456223Z",
     "iopub.status.idle": "2025-10-16T01:58:26.013919Z",
     "shell.execute_reply": "2025-10-16T01:58:26.012334Z"
    },
    "papermill": {
     "duration": 15.564506,
     "end_time": "2025-10-16T01:58:26.016204",
     "exception": false,
     "start_time": "2025-10-16T01:58:10.451698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from pyts.image import GramianAngularField\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80148598",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T01:58:26.025731Z",
     "iopub.status.busy": "2025-10-16T01:58:26.025184Z",
     "iopub.status.idle": "2025-10-16T01:58:26.035713Z",
     "shell.execute_reply": "2025-10-16T01:58:26.034314Z"
    },
    "papermill": {
     "duration": 0.016812,
     "end_time": "2025-10-16T01:58:26.037242",
     "exception": false,
     "start_time": "2025-10-16T01:58:26.020430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Seccessful!\n"
     ]
    }
   ],
   "source": [
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "TARGET_IMAGE_SIZE = 224\n",
    "\n",
    "# 定义常量\n",
    "WINDOW_SIZE = 512  # 减小窗口大小以减少内存使用\n",
    "OVERLAP_RATE = 0.4  # 增加步长以减少生成的图像数量\n",
    "STRIDE = int(WINDOW_SIZE * (1 - OVERLAP_RATE))\n",
    "MAX_IMAGES_PER_COLUMN = 500 # 每列最多生成的图像数量\n",
    "print(\"Seccessful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2285f9c3",
   "metadata": {
    "papermill": {
     "duration": 0.003951,
     "end_time": "2025-10-16T01:58:26.045072",
     "exception": false,
     "start_time": "2025-10-16T01:58:26.041121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.数据预处理\n",
    "## （1）数据转CSV格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d8c9990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T01:58:26.054410Z",
     "iopub.status.busy": "2025-10-16T01:58:26.053700Z",
     "iopub.status.idle": "2025-10-16T01:58:26.099604Z",
     "shell.execute_reply": "2025-10-16T01:58:26.098312Z"
    },
    "papermill": {
     "duration": 0.052007,
     "end_time": "2025-10-16T01:58:26.100946",
     "exception": true,
     "start_time": "2025-10-16T01:58:26.048939",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121265, 10)\n"
     ]
    }
   ],
   "source": [
    "file_names = ['97.mat', '105.mat', '118.mat', '130.mat', '169.mat',\n",
    "              '185.mat', '197.mat', '209.mat', '222.mat', '234.mat']\n",
    "# 采用驱动端数据\n",
    "data_columns = ['X097_DE_time', 'X105_DE_time', 'X118_DE_time', 'X130_DE_time', 'X169_DE_time',\n",
    "                'X185_DE_time','X197_DE_time','X209_DE_time','X222_DE_time','X234_DE_time']\n",
    "columns_name = ['de_normal','de_7_inner','de_7_ball','de_7_outer','de_14_inner','de_14_ball','de_14_outer','de_21_inner','de_21_ball','de_21_outer']\n",
    "data_12k_1797_10c = pd.DataFrame()\n",
    "for index in range(10):\n",
    "    # 读取MAT文件\n",
    "    data = loadmat(f'../data_deal/{file_names[index]}')\n",
    "    dataList = data[data_columns[index]].reshape(-1)\n",
    "    data_12k_1797_10c[columns_name[index]] = dataList[:121265]  # 121048  min: 121265\n",
    "print(data_12k_1797_10c.shape)\n",
    "# # 转换为CSV格式文件\n",
    "# data_12k_1797_10c.set_index('de_normal',inplace=True)\n",
    "# data_12k_1797_10c.to_csv('data_12k_1797_10c.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e8059",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## （2）一维数据转二维图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7f6afb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T05:32:58.277235Z",
     "iopub.status.busy": "2025-10-15T05:32:58.276601Z",
     "iopub.status.idle": "2025-10-15T05:44:46.933338Z",
     "shell.execute_reply": "2025-10-15T05:44:46.932542Z",
     "shell.execute_reply.started": "2025-10-15T05:32:58.277208Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在使用 GASF 处理列: de_normal ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 GASF 处理列: de_7_inner ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 GASF 处理列: de_7_ball ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 GASF 处理列: de_7_outer ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 GASF 处理列: de_14_inner ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 GASF 处理列: de_14_ball ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 GASF 处理列: de_14_outer ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 GASF 处理列: de_21_inner ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 GASF 处理列: de_21_ball ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 GASF 处理列: de_21_outer ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "所有列的 GAF 图像处理完成。请检查 'dataset' 文件夹以查看生成的图像。\n"
     ]
    }
   ],
   "source": [
    "def _data_to_gaf(window_normalized, method='summation'):\n",
    "    \"\"\"\n",
    "    使用 Gramian Angular Field (GAF) 将一维**已归一化**的信号窗口转换为二维矩阵。\n",
    "    \n",
    "    注意：此函数期望输入 'window_normalized' 已经被 Min-Max 归一化到 [-1, 1]。\n",
    "\n",
    "    :param window_normalized: 时域信号窗口 (1D numpy array)，已归一化到 [-1, 1]\n",
    "    :param method: GAF 类型 ('summation' -> GASF, 'difference' -> GADF)\n",
    "    :return: GAF 矩阵 (N x N, N=len(window))\n",
    "    \"\"\"\n",
    "    # 移除 'standardization' 参数，使用默认值或指定 'sample_range'\n",
    "    # GAF 默认使用 [0, pi] 角度范围，要求输入在 [-1, 1] 或 [0, 1] 范围内。\n",
    "    # 我们将在外部手动归一化到 [-1, 1]。\n",
    "    transformer = GramianAngularField(\n",
    "        image_size=len(window_normalized), \n",
    "        method=method\n",
    "        # 移除: standardization='minmax'\n",
    "        # 可以添加 sample_range=(-1., 1.) 确保角度映射正确\n",
    "        # sample_range=(-1., 1.) \n",
    "    )\n",
    "\n",
    "    # pyts 要求输入是 (n_samples, n_timestamps) 形状，所以需要 reshape\n",
    "    X = window_normalized.reshape(1, -1)\n",
    "    \n",
    "    # 进行转换\n",
    "    X_gaf = transformer.fit_transform(X)\n",
    "    \n",
    "    # 返回 GAF 矩阵 (形状为 N x N)\n",
    "    return X_gaf[0]\n",
    "\n",
    "\n",
    "def generate_gaf_images(column_data, window_size, stride, max_images_per_column, target_size=TARGET_IMAGE_SIZE, gaf_method='summation'):\n",
    "    \"\"\"\n",
    "    使用 Gramian Angular Field (GAF) 生成图像。\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    image_count = 0\n",
    "    \n",
    "    if window_size > len(column_data):\n",
    "        print(\"警告: 窗口大小大于数据长度，跳过生成。\")\n",
    "        return images\n",
    "\n",
    "    for start in range(0, len(column_data) - window_size + 1, stride):\n",
    "        if image_count >= max_images_per_column:\n",
    "            break\n",
    "            \n",
    "        window = column_data[start:start + window_size].copy()\n",
    "\n",
    "        # --- 关键修改点 1: 手动 Min-Max 归一化到 [-1, 1] ---\n",
    "        min_val_window = np.min(window)\n",
    "        max_val_window = np.max(window)\n",
    "        \n",
    "        if max_val_window - min_val_window < 1e-6:\n",
    "            # 窗口值几乎不变，归一化到 0\n",
    "            window_normalized = window * 0.0\n",
    "        else:\n",
    "            # 归一化到 [-1, 1]：(2 * (x - min) / (max - min)) - 1\n",
    "            window_normalized = (2 * (window - min_val_window) / (max_val_window - min_val_window)) - 1\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        # 1. 计算 GAF 矩阵 (N x N)\n",
    "        gaf_matrix = _data_to_gaf(window_normalized, method=gaf_method)\n",
    "        \n",
    "        # 2. 归一化到 [0, 1] (将 GAF 值从 [-1, 1] 映射到图像的 [0, 1])\n",
    "        # 这一步是为了图像显示和后续处理的统一。\n",
    "        min_gaf = np.min(gaf_matrix)\n",
    "        max_gaf = np.max(gaf_matrix)\n",
    "        \n",
    "        if max_gaf - min_gaf > 1e-6:\n",
    "            normalized_gaf = (gaf_matrix - min_gaf) / (max_gaf - min_gaf)\n",
    "        else:\n",
    "            normalized_gaf = gaf_matrix * 0.0\n",
    "            \n",
    "        # 3. 图像缩放/插值到目标尺寸\n",
    "        h, w = normalized_gaf.shape\n",
    "        scale = target_size / h \n",
    "\n",
    "        downscaled_image = zoom(normalized_gaf, scale, order=3)\n",
    "        \n",
    "        # 4. 堆叠成 3 通道 (RGB)\n",
    "        image = np.stack((downscaled_image,) * 3, axis=-1)\n",
    "        \n",
    "        images.append((start, image))\n",
    "        image_count += 1\n",
    "        \n",
    "    return images\n",
    "\n",
    "\n",
    "# --- GAF 图像处理与保存函数 (保持不变) ---\n",
    "def process_and_display_column_gaf(data, column_name, window_size, stride, max_images_per_column, target_size=TARGET_IMAGE_SIZE, gaf_method='summation'):\n",
    "    \"\"\" \n",
    "    处理并保存指定列的 GAF 图像。\n",
    "    \"\"\"\n",
    "    method_name = \"GASF\" if gaf_method == 'summation' else \"GADF\"\n",
    "    print(f\"\\n--- 正在使用 {method_name} 处理列: {column_name} ---\")\n",
    "    column_data = data[column_name].values\n",
    "\n",
    "    # 1. 生成 GAF 图像列表\n",
    "    # 这里调用修改后的 generate_gaf_images\n",
    "    gaf_images = generate_gaf_images(column_data, window_size, stride, max_images_per_column, target_size, gaf_method)\n",
    "\n",
    "    # 创建 'dataset' 文件夹\n",
    "    dataset_folder = 'dataset'\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        os.makedirs(dataset_folder)\n",
    "\n",
    "    # 创建列名和 GAF 方法对应的文件夹\n",
    "    column_folder_name = f'{column_name}'\n",
    "    column_folder = os.path.join(dataset_folder, column_folder_name)\n",
    "    if not os.path.exists(column_folder):\n",
    "        os.makedirs(column_folder)\n",
    "\n",
    "    print(f\"生成的图像数量: {len(gaf_images)}\")\n",
    "\n",
    "    # 2. 遍历并保存图像\n",
    "    for image_index, (start, image) in enumerate(gaf_images):\n",
    "        plt.figure(figsize=(2.24, 2.24), dpi=100)\n",
    "        \n",
    "        plt.imshow(image[:, :, 0], cmap='jet', origin='lower', \n",
    "                   vmin=0.0, vmax=1.0) \n",
    "        \n",
    "        plt.title(f'{column_name} {method_name} - Index: {start}', fontsize=8)\n",
    "        plt.axis('off') \n",
    "\n",
    "        image_filename = os.path.join(column_folder, f'{column_name}_{method_name}_{image_index + 1}_S{start}.png')\n",
    "        plt.savefig(image_filename, bbox_inches='tight', pad_inches=0) \n",
    "        plt.close() \n",
    "\n",
    "        if (image_index + 1) % 50 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    # 3. 内存清理\n",
    "    del column_data, gaf_images\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# --- 运行 GAF 图像生成 (保持不变) ---\n",
    "\n",
    "# 1. 生成 GASF 图像\n",
    "for col in data_12k_1797_10c.columns:\n",
    "    process_and_display_column_gaf(data_12k_1797_10c, col, WINDOW_SIZE, STRIDE, MAX_IMAGES_PER_COLUMN, TARGET_IMAGE_SIZE, gaf_method='summation')\n",
    "\n",
    "# # 2. 生成 GADF 图像\n",
    "# for col in data_12k_1797_10c.columns:\n",
    "#     process_and_display_column_gaf(data_12k_1797_10c, col, WINDOW_SIZE, STRIDE, MAX_IMAGES_PER_COLUMN, TARGET_IMAGE_SIZE, gaf_method='difference')\n",
    "\n",
    "print(\"\\n所有列的 GAF 图像处理完成。请检查 'dataset' 文件夹以查看生成的图像。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954109ac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 3.数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f21d9f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T05:44:46.935322Z",
     "iopub.status.busy": "2025-10-15T05:44:46.934772Z",
     "iopub.status.idle": "2025-10-15T05:45:19.192322Z",
     "shell.execute_reply": "2025-10-15T05:45:19.191385Z",
     "shell.execute_reply.started": "2025-10-15T05:44:46.935302Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (3940, 224, 224, 4)\n",
      "Labels shape: (3940,)\n",
      "Train images shape: (2758, 224, 224, 4)\n",
      "Train labels shape: (2758,)\n",
      "Validation images shape: (788, 224, 224, 4)\n",
      "Validation labels shape: (788,)\n",
      "Test images shape: (394, 224, 224, 4)\n",
      "Test labels shape: (394,)\n"
     ]
    }
   ],
   "source": [
    "# 根据需要模型接口调整图像大小\n",
    "IMAGE_SIZE = (224, 224) \n",
    "\n",
    "# 定义数据集路径\n",
    "dataset_path = 'dataset'\n",
    "\n",
    "# 初始化数据列表和标签列表\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# 遍历数据集文件夹\n",
    "for class_name in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            if image_path.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                # 打开并调整图像大小\n",
    "                img = Image.open(image_path).resize(IMAGE_SIZE, Image.LANCZOS)\n",
    "                # 将图像转换为numpy数组\n",
    "                img_array = np.array(img)\n",
    "                # 添加到图像列表\n",
    "                images.append(img_array)\n",
    "                # 添加对应标签\n",
    "                labels.append(class_name)\n",
    "\n",
    "# 将图像数据和标签转换为numpy数组\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 将类别名称转换为数字标签\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# 打印数据和标签的形状\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Labels shape: {labels_encoded.shape}')\n",
    "\n",
    "# 归一化数据\n",
    "images = images.astype('float32')\n",
    "images = images / 255.0\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, labels_encoded, test_size=0.3, random_state=42, stratify=labels_encoded)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42, stratify=y_temp)\n",
    "\n",
    "# 打印划分后的数据集形状\n",
    "print(f'Train images shape: {X_train.shape}')\n",
    "print(f'Train labels shape: {y_train.shape}')\n",
    "print(f'Validation images shape: {X_val.shape}')\n",
    "print(f'Validation labels shape: {y_val.shape}')\n",
    "print(f'Test images shape: {X_test.shape}')\n",
    "print(f'Test labels shape: {y_test.shape}')\n",
    "\n",
    "# # 将数据集保存到文件\n",
    "# np.save('X_train.npy', X_train)\n",
    "# np.save('y_train.npy', y_train)\n",
    "# np.save('X_val.npy', X_val)\n",
    "# np.save('y_val.npy', y_val)\n",
    "# np.save('X_test.npy', X_test)\n",
    "# np.save('y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec531a7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 4.定义数据加载器，导入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c0e489d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:22:14.586633Z",
     "iopub.status.busy": "2025-10-15T07:22:14.585930Z",
     "iopub.status.idle": "2025-10-15T07:22:17.003352Z",
     "shell.execute_reply": "2025-10-15T07:22:17.002665Z",
     "shell.execute_reply.started": "2025-10-15T07:22:14.586607Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded succesfull!\n"
     ]
    }
   ],
   "source": [
    "# # X_train 和 y_train 已经是 numpy 数组\n",
    "# X_train = np.load('/kaggle/working/X_train.npy')\n",
    "# y_train = np.load('/kaggle/working/y_train.npy')\n",
    "# X_val = np.load('/kaggle/working/X_val.npy')\n",
    "# y_val = np.load('/kaggle/working/y_val.npy')\n",
    "\n",
    "# 将 numpy 数组转换为 torch 张量\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "y_val_tensor = torch.from_numpy(y_val).long()\n",
    "\n",
    "# 创建 TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# 定义数据加载器\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Data loaded succesfull!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828fa28e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 5.加载预训练模型，模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49adb228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T08:46:49.325285Z",
     "iopub.status.busy": "2025-10-15T08:46:49.324574Z",
     "iopub.status.idle": "2025-10-15T08:46:49.815115Z",
     "shell.execute_reply": "2025-10-15T08:46:49.814378Z",
     "shell.execute_reply.started": "2025-10-15T08:46:49.325260Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载预训练的效果最好的resnet50权重版本\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# 假设你有 10 个类别\n",
    "num_classes = 10\n",
    "\n",
    "# 冻结所有参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 获取 ResNet50 最后一个全连接层 (model.fc) 的输入特征数\n",
    "# 对于 ResNet50，这个值固定是 2048\n",
    "num_ftrs = model.fc.in_features \n",
    "\n",
    "# 修改分类层 (model.fc) 以适应你的分类任务\n",
    "# 这里使用你提供的更复杂的分类器结构作为示例\n",
    "model.fc = nn.Sequential(\n",
    "    # 第一层从 ResNet50 的输出 (2048) 开始\n",
    "    nn.Linear(num_ftrs, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    \n",
    "    # 输出层：连接到你的类别数\n",
    "    nn.Linear(2048 , num_classes)\n",
    ")\n",
    "\n",
    "# 将模型移动到GPU（如果可用）\n",
    "model.to(device)\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af318990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T08:47:24.818728Z",
     "iopub.status.busy": "2025-10-15T08:47:24.818025Z",
     "iopub.status.idle": "2025-10-15T08:47:24.824607Z",
     "shell.execute_reply": "2025-10-15T08:47:24.823785Z",
     "shell.execute_reply.started": "2025-10-15T08:47:24.818703Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful!\n"
     ]
    }
   ],
   "source": [
    "# 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 自定义优化器和学习率调度器\n",
    "LR_CUSTOM_HEAD = 1e-4  # 0.0001 (自定义隐藏层)\n",
    "LR_LAYER4 = 1e-5       # 0.00001 (解冻的 layer4)\n",
    "param_groups = [\n",
    "    # 参数组 1: 自定义 FC Head\n",
    "    {'params': model.fc.parameters(), 'lr': LR_CUSTOM_HEAD, 'group_name': 'custom_head'},\n",
    "    \n",
    "    # 参数组 2: 解冻的 layer4\n",
    "    {'params': model.layer4.parameters(), 'lr': LR_LAYER4, 'group_name': 'layer4_fine_tune'}\n",
    "]\n",
    "optimizer = optim.Adam(param_groups)\n",
    "\n",
    "# 定义总训练周期和每个周期的批次数量\n",
    "num_epochs = 200\n",
    "steps_per_epoch = len(train_loader) \n",
    "\n",
    "# OneCycleLR 的 max_lr 参数现在是一个列表，对应优化器中每个参数组的最大学习率\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=[LR_CUSTOM_HEAD, LR_LAYER4],\n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=num_epochs\n",
    ")\n",
    "print(\"Successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18df706",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 6.模型训练和验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5f8524f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T08:47:28.068941Z",
     "iopub.status.busy": "2025-10-15T08:47:28.068679Z",
     "iopub.status.idle": "2025-10-15T10:00:46.157579Z",
     "shell.execute_reply": "2025-10-15T10:00:46.156764Z",
     "shell.execute_reply.started": "2025-10-15T08:47:28.068921Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 2.2770, Train Accuracy: 0.1682\n",
      "Validation Loss: 2.2477, Validation Accuracy: 0.3832\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 2/200, Train Loss: 2.2189, Train Accuracy: 0.3778\n",
      "Validation Loss: 2.1897, Validation Accuracy: 0.6015\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 3/200, Train Loss: 2.1547, Train Accuracy: 0.5395\n",
      "Validation Loss: 2.1247, Validation Accuracy: 0.7183\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 4/200, Train Loss: 2.0830, Train Accuracy: 0.6095\n",
      "Validation Loss: 2.0388, Validation Accuracy: 0.7348\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 5/200, Train Loss: 1.9812, Train Accuracy: 0.6831\n",
      "Validation Loss: 1.9344, Validation Accuracy: 0.7805\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 6/200, Train Loss: 1.8606, Train Accuracy: 0.7259\n",
      "Validation Loss: 1.7973, Validation Accuracy: 0.7868\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 7/200, Train Loss: 1.7116, Train Accuracy: 0.7389\n",
      "Validation Loss: 1.6379, Validation Accuracy: 0.8147\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 8/200, Train Loss: 1.5268, Train Accuracy: 0.7788\n",
      "Validation Loss: 1.4354, Validation Accuracy: 0.8261\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 9/200, Train Loss: 1.3495, Train Accuracy: 0.7883\n",
      "Validation Loss: 1.2554, Validation Accuracy: 0.8490\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 10/200, Train Loss: 1.1539, Train Accuracy: 0.8151\n",
      "Validation Loss: 1.0439, Validation Accuracy: 0.8718\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 11/200, Train Loss: 0.9704, Train Accuracy: 0.8441\n",
      "Validation Loss: 0.8672, Validation Accuracy: 0.8845\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 12/200, Train Loss: 0.7897, Train Accuracy: 0.8680\n",
      "Validation Loss: 0.6928, Validation Accuracy: 0.9036\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 13/200, Train Loss: 0.6080, Train Accuracy: 0.9061\n",
      "Validation Loss: 0.5313, Validation Accuracy: 0.9353\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 14/200, Train Loss: 0.4618, Train Accuracy: 0.9351\n",
      "Validation Loss: 0.3928, Validation Accuracy: 0.9505\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 15/200, Train Loss: 0.3506, Train Accuracy: 0.9449\n",
      "Validation Loss: 0.2872, Validation Accuracy: 0.9721\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 16/200, Train Loss: 0.2532, Train Accuracy: 0.9608\n",
      "Validation Loss: 0.2119, Validation Accuracy: 0.9810\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 17/200, Train Loss: 0.1851, Train Accuracy: 0.9772\n",
      "Validation Loss: 0.1395, Validation Accuracy: 0.9835\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 18/200, Train Loss: 0.1382, Train Accuracy: 0.9851\n",
      "Validation Loss: 0.1020, Validation Accuracy: 0.9860\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 19/200, Train Loss: 0.0953, Train Accuracy: 0.9877\n",
      "Validation Loss: 0.0781, Validation Accuracy: 0.9911\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 20/200, Train Loss: 0.0678, Train Accuracy: 0.9924\n",
      "Validation Loss: 0.0558, Validation Accuracy: 0.9911\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 21/200, Train Loss: 0.0528, Train Accuracy: 0.9946\n",
      "Validation Loss: 0.0420, Validation Accuracy: 0.9975\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 22/200, Train Loss: 0.0381, Train Accuracy: 0.9956\n",
      "Validation Loss: 0.0339, Validation Accuracy: 0.9962\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 23/200, Train Loss: 0.0312, Train Accuracy: 0.9964\n",
      "Validation Loss: 0.0267, Validation Accuracy: 0.9975\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 24/200, Train Loss: 0.0229, Train Accuracy: 0.9982\n",
      "Validation Loss: 0.0209, Validation Accuracy: 1.0000\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 25/200, Train Loss: 0.0186, Train Accuracy: 0.9982\n",
      "Validation Loss: 0.0167, Validation Accuracy: 1.0000\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 26/200, Train Loss: 0.0164, Train Accuracy: 0.9985\n",
      "Validation Loss: 0.0168, Validation Accuracy: 0.9987\n",
      "验证损失未改善，耐心计数: 1/40\n",
      "Epoch 27/200, Train Loss: 0.0131, Train Accuracy: 1.0000\n",
      "Validation Loss: 0.0121, Validation Accuracy: 1.0000\n",
      "验证损失降低，保存最佳模型。\n",
      "Epoch 28/200, Train Loss: 0.0103, Train Accuracy: 0.9996\n",
      "Validation Loss: 0.0095, Validation Accuracy: 1.0000\n",
      "\n",
      "✨ 验证损失 0.0095 达到或低于阈值 0.01，停止训练。\n",
      "模型已保存。\n"
     ]
    }
   ],
   "source": [
    "# --- 早停策略配置 ---\n",
    "patience = 40 # 连续10个epoch验证损失没有改善就停止\n",
    "best_val_loss = float('inf') # 初始最佳验证损失设置为无穷大\n",
    "BEST_LOSS_THRESHOLD = 0.01 # 新增的停止阈值\n",
    "epochs_no_improve = 0 # 记录没有改善的epoch数量\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        inputs = inputs[:, :3, :, :]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.permute(0, 3, 1, 2)\n",
    "            inputs = inputs[:, :3, :, :]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    val_epoch_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f'Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 策略 1: 检查是否达到预设的最低损失阈值\n",
    "    if val_epoch_loss <= BEST_LOSS_THRESHOLD:\n",
    "        print(f\"\\n✨ 验证损失 {val_epoch_loss:.4f} 达到或低于阈值 {BEST_LOSS_THRESHOLD}，停止训练。\")\n",
    "        torch.save(model.state_dict(), \"best_resnet50_model.pth\")\n",
    "        print(\"模型已保存。\")\n",
    "        break\n",
    "        \n",
    "    # 策略 2: 标准的基于 patience 的早停\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        # 保存最佳模型\n",
    "        torch.save(model.state_dict(), \"best_resnet50_model.pth\")\n",
    "        print(\"验证损失降低，保存最佳模型。\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"验证损失未改善，耐心计数: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"\\n连续 {patience} 个epoch验证损失没有改善，停止训练。\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d86aa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 7.模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37b98301",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:22:40.646202Z",
     "iopub.status.busy": "2025-10-15T10:22:40.645392Z",
     "iopub.status.idle": "2025-10-15T10:22:41.755636Z",
     "shell.execute_reply": "2025-10-15T10:22:41.754835Z",
     "shell.execute_reply.started": "2025-10-15T10:22:40.646173Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在加载模型文件：best_resnet50_model.pth\n",
      "模型加载完成。\n",
      "\n",
      "开始进行最终测试...\n",
      "Test Loss: 0.0114, Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# # 加载 numpy 数组\n",
    "# X_test = np.load('/kaggle/working/X_test.npy')\n",
    "# y_test = np.load('/kaggle/working/y_test.npy')\n",
    "\n",
    "# 将 numpy 数组转换为 torch 张量\n",
    "# X_test 转换为 float 类型以匹配模型输入\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "# y_test 转换为 long 类型以匹配 CrossEntropyLoss 的期望\n",
    "y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 创建 TensorDataset\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# 定义数据加载器\n",
    "batch_size = 32\n",
    "# 测试集通常不需要打乱，因此 shuffle=False\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "MODEL_PATH = \"best_resnet50_model.pth\"\n",
    "print(f\"\\n正在加载模型文件：{MODEL_PATH}\")\n",
    "# 使用 map_location 将模型加载到正确的设备上\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "print(\"模型加载完成。\")\n",
    "\n",
    "print(\"\\n开始进行最终测试...\")\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # 数据预处理\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        inputs = inputs[:, :3, :, :]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # 收集所有真实标签和预测标签\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "test_accuracy = np.mean(np.array(all_labels) == np.array(all_preds))\n",
    "test_epoch_loss = test_loss / len(test_loader)\n",
    "print(f'Test Loss: {test_epoch_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88875c1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8270633,
     "sourceId": 13060558,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "1Dto2D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23.216818,
   "end_time": "2025-10-16T01:58:28.987040",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-16T01:58:05.770222",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
