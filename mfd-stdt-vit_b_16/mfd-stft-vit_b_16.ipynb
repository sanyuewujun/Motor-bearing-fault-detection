{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:12:05.851065Z",
     "iopub.status.busy": "2025-10-24T05:12:05.850810Z",
     "iopub.status.idle": "2025-10-24T05:12:05.855959Z",
     "shell.execute_reply": "2025-10-24T05:12:05.855099Z",
     "shell.execute_reply.started": "2025-10-24T05:12:05.851047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import stft\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:12:05.856594Z",
     "iopub.status.busy": "2025-10-24T05:12:05.856432Z",
     "iopub.status.idle": "2025-10-24T05:12:05.873702Z",
     "shell.execute_reply": "2025-10-24T05:12:05.872909Z",
     "shell.execute_reply.started": "2025-10-24T05:12:05.856568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "successful!\n"
     ]
    }
   ],
   "source": [
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# 定义参数\n",
    "FS = 512\n",
    "TARGET_IMAGE_SIZE = 224\n",
    "# 定义常量\n",
    "WINDOW_SIZE = 512  # 减小窗口大小以减少内存使用\n",
    "OVERLAP_RATE = 0.4  # 增加步长以减少生成的图像数量\n",
    "STRIDE = int(WINDOW_SIZE * (1 - OVERLAP_RATE))\n",
    "MAX_IMAGES_PER_COLUMN = 500 # 每列最多生成的图像数量\n",
    "print('successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据预处理\n",
    "## （1）数据转CSV格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:12:05.894702Z",
     "iopub.status.busy": "2025-10-24T05:12:05.894495Z",
     "iopub.status.idle": "2025-10-24T05:12:06.330407Z",
     "shell.execute_reply": "2025-10-24T05:12:06.329365Z",
     "shell.execute_reply.started": "2025-10-24T05:12:05.894685Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121265, 10)\n"
     ]
    }
   ],
   "source": [
    "file_names = ['97.mat', '105.mat', '118.mat', '130.mat', '169.mat',\n",
    "              '185.mat', '197.mat', '209.mat', '222.mat', '234.mat']\n",
    "# 采用驱动端数据\n",
    "data_columns = ['X097_DE_time', 'X105_DE_time', 'X118_DE_time', 'X130_DE_time', 'X169_DE_time',\n",
    "                'X185_DE_time','X197_DE_time','X209_DE_time','X222_DE_time','X234_DE_time']\n",
    "columns_name = ['de_normal','de_7_inner','de_7_ball','de_7_outer','de_14_inner','de_14_ball','de_14_outer','de_21_inner','de_21_ball','de_21_outer']\n",
    "data_12k_1797_10c = pd.DataFrame()\n",
    "for index in range(10):\n",
    "    # 读取MAT文件\n",
    "    data = loadmat(f'../data_deal/{file_names[index]}')\n",
    "    dataList = data[data_columns[index]].reshape(-1)\n",
    "    data_12k_1797_10c[columns_name[index]] = dataList[:121265]  # 121048  min: 121265\n",
    "print(data_12k_1797_10c.shape)\n",
    "# # 转换为CSV格式文件\n",
    "# data_12k_1797_10c.set_index('de_normal',inplace=True)\n",
    "# data_12k_1797_10c.to_csv('data_12k_1797_10c.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （2）一维数据转二维图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:12:06.331225Z",
     "iopub.status.busy": "2025-10-24T05:12:06.331054Z",
     "iopub.status.idle": "2025-10-24T05:17:31.427174Z",
     "shell.execute_reply": "2025-10-24T05:17:31.426120Z",
     "shell.execute_reply.started": "2025-10-24T05:12:06.331211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在使用 STFT 处理列: de_normal ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 STFT 处理列: de_7_inner ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 STFT 处理列: de_7_ball ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 STFT 处理列: de_7_outer ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 STFT 处理列: de_14_inner ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 STFT 处理列: de_14_ball ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 STFT 处理列: de_14_outer ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 STFT 处理列: de_21_inner ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 STFT 处理列: de_21_ball ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "--- 正在使用 STFT 处理列: de_21_outer ---\n",
      "生成的图像数量: 394\n",
      "\n",
      "所有列处理完成。请检查 'dataset' 文件夹以查看生成的 STFT 频谱图图像。\n"
     ]
    }
   ],
   "source": [
    "def _stft_to_spectrogram(window, fs=FS):\n",
    "    \"\"\"\n",
    "    使用短时傅里叶变换 (STFT) 计算信号的对数幅度谱图。\n",
    "    \n",
    "    :param window: 时域信号窗口 (1D numpy array)\n",
    "    :param fs: 采样频率\n",
    "    :return: 幅度谱矩阵 (频率维 x 时间维)\n",
    "    \"\"\"\n",
    "    # 使用 STFT 计算：f: 频率轴, t: 时间轴, Zxx: STFT 结果 (复数)\n",
    "    # nperseg 通常取 window_size，但为了简单和保持时间分辨率，此处使用默认。\n",
    "    # 我们可以设置 nperseg=len(window)，然后 nfft 也可以是 len(window)\n",
    "    f, t, Zxx = stft(window, fs=fs, nperseg=len(window), noverlap=0, nfft=len(window))\n",
    "    \n",
    "    # 取幅度，通常会取对数幅度 (dB) 来增强视觉效果\n",
    "    # 转换为对数尺度 (分贝)\n",
    "    spectrogram = np.log1p(np.abs(Zxx)) \n",
    "    \n",
    "    # STFT 默认返回的是双边谱，但通常我们只关心单边谱 (0 到 Nyquist 频率)\n",
    "    # nperseg=len(window) 时，STFT 返回的频率维度是 len(window)//2 + 1\n",
    "    # stft(..., nperseg=N) 返回 N/2 + 1 个频率点。\n",
    "    # Zxx 的维度是 (freq_bins, time_steps)\n",
    "    \n",
    "    # 由于 nperseg=len(window) 且 noverlap=0，time_steps 只有 1。\n",
    "    # 这不是我们想要的。我们需要 STFT 在窗口内滑动。\n",
    "    \n",
    "    # 重新计算 STFT，以获得多个时间步\n",
    "    # 我们希望 spectrogram 的形状是 (频率维, 时间维)，其中时间维要比 1 大。\n",
    "    # 设置 nperseg 为一个较小的值（例如 1/8 窗口长度），并设置重叠\n",
    "    nperseg = len(window) // 8\n",
    "    noverlap = nperseg // 2\n",
    "    \n",
    "    f, t, Zxx = stft(window, fs=fs, nperseg=nperseg, noverlap=noverlap, nfft=nperseg)\n",
    "    \n",
    "    # 转换为对数尺度\n",
    "    # 加上一个很小的数避免 log(0)\n",
    "    spectrogram = np.log1p(np.abs(Zxx) + 1e-10)\n",
    "    \n",
    "    # 只取单边谱 (前半部分频率，包含 DC 分量)\n",
    "    # Zxx 的维度是 (nperseg//2 + 1, num_time_slices)\n",
    "    # spectrogram = spectrogram[:nperseg//2 + 1, :] # 已经是这个形状\n",
    "    \n",
    "    return spectrogram\n",
    "\n",
    "\n",
    "def generate_stft_images(column_data, window_size, stride, max_images_per_column, target_size=TARGET_IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    使用短时傅里叶变换（STFT）生成频谱图图像。\n",
    "    \n",
    "    :param column_data: 列数据（1D numpy array）\n",
    "    :param window_size: 滑动窗口大小 (用于提取数据片段)\n",
    "    :param stride: 步长\n",
    "    :param max_images_per_column: 每列最多生成的图像数量\n",
    "    :param target_size: 降采样后的目标图像尺寸\n",
    "    :return: 包含 (起始索引, 图像数组) 的列表\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    image_count = 0\n",
    "    \n",
    "    for start in range(0, len(column_data) - window_size + 1, stride):\n",
    "        if image_count >= max_images_per_column:\n",
    "            break\n",
    "            \n",
    "        window = column_data[start:start + window_size].copy()\n",
    "        \n",
    "        # 1. 计算 STFT 频谱图\n",
    "        # spectrogram 矩阵维度: (频率维, 时间维)\n",
    "        spectrogram = _stft_to_spectrogram(window)\n",
    "        \n",
    "        # 2. 归一化到 [0, 1]\n",
    "        min_val = np.min(spectrogram)\n",
    "        max_val = np.max(spectrogram)\n",
    "        if max_val - min_val > 1e-6:\n",
    "            normalized_spec = (spectrogram - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            normalized_spec = spectrogram * 0.0\n",
    "\n",
    "        # 3. 图像缩放/插值到目标尺寸 (目标尺寸是 TARGET_IMAGE_SIZE x TARGET_IMAGE_SIZE)\n",
    "        h, w = normalized_spec.shape\n",
    "        \n",
    "        # STFT 时间维 w 依赖于 nperseg 和 noverlap，通常 w << window_size。\n",
    "        # H 是 STFT 的频率维 (nperseg//2 + 1)。\n",
    "        \n",
    "        # 计算缩放比例： target_size / actual_size\n",
    "        scale_h = target_size / h # 频率轴缩放比例\n",
    "        scale_w = target_size / w # 时间轴缩放比例\n",
    "        \n",
    "        # 使用三次插值进行缩放\n",
    "        # zoom 会自动处理目标尺寸不整除的情况，结果会是 (target_size, target_size)\n",
    "        downscaled_image = zoom(normalized_spec, (scale_h, scale_w), order=3) \n",
    "\n",
    "        # 4. 堆叠成 3 通道 (RGB)，用于 CNN 输入\n",
    "        image = np.stack((downscaled_image,) * 3, axis=-1)\n",
    "        \n",
    "        images.append((start, image))\n",
    "        image_count += 1\n",
    "        \n",
    "    return images\n",
    "\n",
    "\n",
    "# --- 4. 图像处理与保存函数 (修改名称和输出信息) ---\n",
    "\n",
    "def process_and_display_column(data, column_name, window_size, stride, max_images_per_column, target_size=TARGET_IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    处理并保存指定列的 STFT 频谱图图像。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 正在使用 STFT 处理列: {column_name} ---\")\n",
    "    column_data = data[column_name].values\n",
    "    \n",
    "    # 1. 生成 STFT 图像列表\n",
    "    stft_images = generate_stft_images(column_data, window_size, stride, max_images_per_column, target_size)\n",
    "    \n",
    "    # 创建 'dataset' 文件夹\n",
    "    dataset_folder = 'dataset'\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        os.makedirs(dataset_folder)\n",
    "    \n",
    "    # 创建列名对应的文件夹\n",
    "    column_folder = os.path.join(dataset_folder, column_name)\n",
    "    if not os.path.exists(column_folder):\n",
    "        os.makedirs(column_folder)\n",
    "    \n",
    "    print(f\"生成的图像数量: {len(stft_images)}\")\n",
    "\n",
    "    # 2. 遍历并保存图像\n",
    "    for image_index, (start, image) in enumerate(stft_images):\n",
    "        plt.figure(figsize=(2.24, 2.24)) \n",
    "        # 频谱图通常也用热力图，且通常低频在底部 (origin='lower')\n",
    "        plt.imshow(image[:, :, 0], cmap='jet', origin='lower') # 换一个 STFT 常用色图\n",
    "        plt.title(f'{column_name} STFT - Index: {start}', fontsize=8)\n",
    "        plt.axis('off') # 不显示坐标轴\n",
    "        \n",
    "        # 保存图像到指定文件夹\n",
    "        image_filename = os.path.join(column_folder, f'{column_name}_STFT_{image_index + 1}_S{start}.png')\n",
    "        plt.savefig(image_filename, bbox_inches='tight', pad_inches=0.1) \n",
    "        plt.close() # 立即关闭 figure，释放内存\n",
    "        \n",
    "        # 内存管理\n",
    "        if (image_index + 1) % 50 == 0:\n",
    "            gc.collect() \n",
    "    \n",
    "    # 3. 内存清理\n",
    "    del column_data, stft_images\n",
    "    gc.collect()\n",
    "\n",
    "# 处理每一列的数据 (保持原代码结构)\n",
    "for col in data_12k_1797_10c.columns:\n",
    "    process_and_display_column(data_12k_1797_10c, col, WINDOW_SIZE, STRIDE, MAX_IMAGES_PER_COLUMN)\n",
    "\n",
    "print(\"\\n所有列处理完成。请检查 'dataset' 文件夹以查看生成的 STFT 频谱图图像。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:17:31.428020Z",
     "iopub.status.busy": "2025-10-24T05:17:31.427853Z",
     "iopub.status.idle": "2025-10-24T05:17:42.126291Z",
     "shell.execute_reply": "2025-10-24T05:17:42.125268Z",
     "shell.execute_reply.started": "2025-10-24T05:17:31.428005Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (3940, 224, 224, 4)\n",
      "Labels shape: (3940,)\n",
      "Train images shape: (2758, 224, 224, 4)\n",
      "Train labels shape: (2758,)\n",
      "Validation images shape: (788, 224, 224, 4)\n",
      "Validation labels shape: (788,)\n",
      "Test images shape: (394, 224, 224, 4)\n",
      "Test labels shape: (394,)\n"
     ]
    }
   ],
   "source": [
    "# 根据需要模型接口调整图像大小\n",
    "IMAGE_SIZE = (224, 224) \n",
    "\n",
    "# 定义数据集路径\n",
    "dataset_path = 'dataset'\n",
    "\n",
    "# 初始化数据列表和标签列表\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# 遍历数据集文件夹\n",
    "for class_name in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            if image_path.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                # 打开并调整图像大小\n",
    "                img = Image.open(image_path).resize(IMAGE_SIZE, Image.LANCZOS)\n",
    "                # 将图像转换为numpy数组\n",
    "                img_array = np.array(img)\n",
    "                # 添加到图像列表\n",
    "                images.append(img_array)\n",
    "                # 添加对应标签\n",
    "                labels.append(class_name)\n",
    "\n",
    "# 将图像数据和标签转换为numpy数组\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 将类别名称转换为数字标签\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# 打印数据和标签的形状\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Labels shape: {labels_encoded.shape}')\n",
    "\n",
    "# 归一化数据\n",
    "images = images.astype('float32')\n",
    "images = images / 255.0\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, labels_encoded, test_size=0.3, random_state=42, stratify=labels_encoded)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42, stratify=y_temp)\n",
    "\n",
    "# 打印划分后的数据集形状\n",
    "print(f'Train images shape: {X_train.shape}')\n",
    "print(f'Train labels shape: {y_train.shape}')\n",
    "print(f'Validation images shape: {X_val.shape}')\n",
    "print(f'Validation labels shape: {y_val.shape}')\n",
    "print(f'Test images shape: {X_test.shape}')\n",
    "print(f'Test labels shape: {y_test.shape}')\n",
    "\n",
    "# # 将数据集保存到文件\n",
    "# np.save('X_train.npy', X_train)\n",
    "# np.save('y_train.npy', y_train)\n",
    "# np.save('X_val.npy', X_val)\n",
    "# np.save('y_val.npy', y_val)\n",
    "# np.save('X_test.npy', X_test)\n",
    "# np.save('y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.定义数据加载器，导入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:17:42.127077Z",
     "iopub.status.busy": "2025-10-24T05:17:42.126897Z",
     "iopub.status.idle": "2025-10-24T05:17:42.132222Z",
     "shell.execute_reply": "2025-10-24T05:17:42.131352Z",
     "shell.execute_reply.started": "2025-10-24T05:17:42.127059Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded succesfull!\n"
     ]
    }
   ],
   "source": [
    "# # X_train 和 y_train 已经是 numpy 数组\n",
    "# X_train = np.load('/kaggle/working/X_train.npy')\n",
    "# y_train = np.load('/kaggle/working/y_train.npy')\n",
    "# X_val = np.load('/kaggle/working/X_val.npy')\n",
    "# y_val = np.load('/kaggle/working/y_val.npy')\n",
    "\n",
    "# 将 numpy 数组转换为 torch 张量\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "y_val_tensor = torch.from_numpy(y_val).long()\n",
    "\n",
    "# 创建 TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# 定义数据加载器\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Data loaded succesfull!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.加载预训练模型，模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:28:13.158497Z",
     "iopub.status.busy": "2025-10-24T05:28:13.158219Z",
     "iopub.status.idle": "2025-10-24T05:28:13.800282Z",
     "shell.execute_reply": "2025-10-24T05:28:13.799163Z",
     "shell.execute_reply.started": "2025-10-24T05:28:13.158478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=768, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "模型总参数量: 87.17 M\n",
      "可训练参数量: 0.60 M\n",
      "微调层参数量 (model.head): 0.60 M\n"
     ]
    }
   ],
   "source": [
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 假设你有 10 个类别\n",
    "num_classes = 10 \n",
    "\n",
    "# 使用效果最好的 ViT-B/16 预训练权重版本\n",
    "model = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "# 冻结所有参数 (特征提取阶段)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# 解冻最后一个 Block (索引 11) 的参数\n",
    "for param in model.encoder.layers[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "print(\"注意：已解冻最后一个 Transformer 编码器块的参数。\")\n",
    "\n",
    "# 获取 ViT 分类头 (model.head) 的输入特征数\n",
    "# 对于 ViT-B/16，这个值是 768\n",
    "num_ftrs = 768\n",
    "\n",
    "# 修改分类头 (model.head) 以适应你的分类任务\n",
    "# ViT 的分类头就是模型在 `cls` token 上接的一个全连接层\n",
    "model.head = nn.Sequential(\n",
    "    # 第一层从 ViT 的输出 (768) 开始\n",
    "    nn.Linear(num_ftrs, 768),  # 注意：这里我们使用 2048 来匹配你 ResNet 示例中的结构\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    \n",
    "    # 输出层：连接到你的类别数\n",
    "    nn.Linear(768, num_classes)\n",
    ")\n",
    "\n",
    "# 将模型移动到GPU（如果可用）\n",
    "model.to(device)\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "\n",
    "# 打印可训练的参数数量 (用于确认冻结策略是否生效)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n模型总参数量: {total_params / 1e6:.2f} M\")\n",
    "print(f\"可训练参数量: {trainable_params / 1e6:.2f} M\")\n",
    "print(f\"微调层参数量 (model.head): {sum(p.numel() for p in model.head.parameters()) / 1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:28:55.852297Z",
     "iopub.status.busy": "2025-10-24T05:28:55.852014Z",
     "iopub.status.idle": "2025-10-24T05:28:55.857945Z",
     "shell.execute_reply": "2025-10-24T05:28:55.857008Z",
     "shell.execute_reply.started": "2025-10-24T05:28:55.852277Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful! Vision Transformer 学习率调度策略已设置。\n"
     ]
    }
   ],
   "source": [
    "# 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 自定义优化器和学习率调度器\n",
    "# ViT 通常使用更低的 LR。\n",
    "LR_CUSTOM_HEAD = 1e-4 # 0.0001 (自定义分类头，可以稍高)\n",
    "LR_LAST_BLOCK = 1e-5   # 0.00001 (解冻的最后一个 Transformer Block，非常低)\n",
    "\n",
    "param_groups = [\n",
    "    # 参数组 1: 自定义分类头 (model.head)\n",
    "    # 这部分是新初始化的，使用相对较高的 LR\n",
    "    {'params': model.head.parameters(), 'lr': LR_CUSTOM_HEAD, 'group_name': 'custom_head'},\n",
    "    \n",
    "    # 参数组 2: 解冻的最后一个 Transformer 编码器块 (model.encoder.layers[-1])\n",
    "    # 这部分是预训练的，使用非常低的 LR 进行微调\n",
    "    {'params': model.encoder.layers[-1].parameters(), 'lr': LR_LAST_BLOCK, 'group_name': 'last_block_fine_tune'}\n",
    "]\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    param_groups, \n",
    "    weight_decay=1e-4 # 应用于所有参数组\n",
    ")\n",
    "\n",
    "# 定义总训练周期和每个周期的批次数量\n",
    "num_epochs = 200\n",
    "steps_per_epoch = len(train_loader) \n",
    "\n",
    "# OneCycleLR 的 max_lr 参数：对应优化器中每个参数组的最大学习率\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=[LR_CUSTOM_HEAD, LR_LAST_BLOCK], # 顺序必须与 param_groups 保持一致\n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=num_epochs\n",
    ")\n",
    "\n",
    "print(\"Successful! Vision Transformer 学习率调度策略已设置。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.模型训练和验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:29:00.347464Z",
     "iopub.status.busy": "2025-10-24T05:29:00.347209Z",
     "iopub.status.idle": "2025-10-24T05:29:01.515068Z",
     "shell.execute_reply": "2025-10-24T05:29:01.513730Z",
     "shell.execute_reply.started": "2025-10-24T05:29:00.347449Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m outputs = model(inputs)\n\u001b[32m     24\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m optimizer.step()\n\u001b[32m     28\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# --- 早停策略配置 ---\n",
    "patience = 40 # 连续10个epoch验证损失没有改善就停止\n",
    "best_val_loss = float('inf') # 初始最佳验证损失设置为无穷大\n",
    "BEST_LOSS_THRESHOLD = 0.01 # 新增的停止阈值\n",
    "epochs_no_improve = 0 # 记录没有改善的epoch数量\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        inputs = inputs[:, :3, :, :]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.permute(0, 3, 1, 2)\n",
    "            inputs = inputs[:, :3, :, :]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    val_epoch_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f'Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 策略 1: 检查是否达到预设的最低损失阈值\n",
    "    if val_epoch_loss <= BEST_LOSS_THRESHOLD:\n",
    "        print(f\"\\n✨ 验证损失 {val_epoch_loss:.4f} 达到或低于阈值 {BEST_LOSS_THRESHOLD}，停止训练。\")\n",
    "        torch.save(model.state_dict(), \"best_vit_model.pth\")\n",
    "        print(\"模型已保存。\")\n",
    "        break\n",
    "        \n",
    "    # 策略 2: 标准的基于 patience 的早停\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        # 保存最佳模型\n",
    "        torch.save(model.state_dict(), \"best_vit_model.pth\")\n",
    "        print(\"验证损失降低，保存最佳模型。\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"验证损失未改善，耐心计数: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"\\n连续 {patience} 个epoch验证损失没有改善，停止训练。\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-24T05:17:42.884668Z",
     "iopub.status.idle": "2025-10-24T05:17:42.885074Z",
     "shell.execute_reply": "2025-10-24T05:17:42.884777Z",
     "shell.execute_reply.started": "2025-10-24T05:17:42.884768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # 加载 numpy 数组\n",
    "# X_test = np.load('/kaggle/working/X_test.npy')\n",
    "# y_test = np.load('/kaggle/working/y_test.npy')\n",
    "\n",
    "# 将 numpy 数组转换为 torch 张量\n",
    "# X_test 转换为 float 类型以匹配模型输入\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "# y_test 转换为 long 类型以匹配 CrossEntropyLoss 的期望\n",
    "y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 创建 TensorDataset\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# 定义数据加载器\n",
    "batch_size = 32\n",
    "# 测试集通常不需要打乱，因此 shuffle=False\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "MODEL_PATH = \"best_vit_model.pth\"\n",
    "print(f\"\\n正在加载模型文件：{MODEL_PATH}\")\n",
    "# 使用 map_location 将模型加载到正确的设备上\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "print(\"模型加载完成。\")\n",
    "\n",
    "print(\"\\n开始进行最终测试...\")\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # 数据预处理\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        inputs = inputs[:, :3, :, :]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # 收集所有真实标签和预测标签\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "test_accuracy = np.mean(np.array(all_labels) == np.array(all_preds))\n",
    "test_epoch_loss = test_loss / len(test_loader)\n",
    "print(f'Test Loss: {test_epoch_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "datasetId": 8270633,
     "sourceId": 13060558,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31155,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
